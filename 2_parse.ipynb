{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a667c254",
   "metadata": {},
   "source": [
    "# LaTeXpOsEd: Parsing Stage\n",
    "\n",
    "In this stage, the downloaded latex files are merged into one JSON list file with only the filtered comments. The comments are filtered for boilerplate, excessive white space, purely separator characters and others.\n",
    "\n",
    "Before running this script:\n",
    "\n",
    "- Complete: [1_scrape.ipynb](1_scrape.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q urlextract tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19865fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPERS_FOLDER = 'data/final'\n",
    "COMMON_COMMENTS_TXT = 'tmp/common_comments.txt'\n",
    "COMMENTS_JSONL = 'data/paper_comments.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49522dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latex_comment_blocks(latex_code: str) -> list:\n",
    "    matches = re.findall(r'\\\\begin\\{comment\\}(.*?)\\\\end\\{comment\\}', latex_code, re.DOTALL | re.IGNORECASE)\n",
    "    return [ match.strip() for match in matches if match.strip() ]\n",
    "\n",
    "def extract_latex_comment_lines(latex_code: str) -> list:\n",
    "    lines = latex_code.splitlines()\n",
    "    in_verbatim = False\n",
    "    comments = []\n",
    "\n",
    "    # Regex for detecting start and end of verbatim-like environments\n",
    "    verbatim_start = re.compile(r'\\\\begin\\{(verbatim|lstlisting|Verbatim)\\}')\n",
    "    verbatim_end = re.compile(r'\\\\end\\{(verbatim|lstlisting|Verbatim)\\}')\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Handle entering or leaving verbatim\n",
    "        if verbatim_start.search(stripped_line):\n",
    "            in_verbatim = True\n",
    "        if verbatim_end.search(stripped_line):\n",
    "            in_verbatim = False\n",
    "            continue\n",
    "        if in_verbatim:\n",
    "            continue\n",
    "\n",
    "        # Find first unescaped `%`\n",
    "        i = 0\n",
    "        while i < len(line):\n",
    "            if line[i] == '%':\n",
    "                if i > 0 and line[i - 1] == '\\\\':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                # Extract comment without `%` and leading whitespace\n",
    "                comments.append(line[i+1:].strip())\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    return comments\n",
    "\n",
    "def extract_latex_comments(latex_code: str) -> list:\n",
    "    comments = extract_latex_comment_lines(latex_code) + extract_latex_comment_blocks(latex_code)\n",
    "    \n",
    "    comments = [comment for comment in comments if comment.strip()] # Remove empty comments\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4723d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all json files in folder\n",
    "for paper_name in os.listdir(PAPERS_FOLDER):\n",
    "    if paper_name.endswith('.json'):\n",
    "        filepath = os.path.join(PAPERS_FOLDER, paper_name)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            files = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f766be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator class for more convenient iteration of the dataset\n",
    "class PaperContentIterator():\n",
    "    def __init__(self, papers_folder: str):\n",
    "        self.papers_folder = papers_folder\n",
    "        self.paper_files = [f for f in os.listdir(papers_folder) if f.endswith('.json')]\n",
    "        self.iteration_count = len(self.paper_files)\n",
    "        self.current_paper_index = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.iteration_count\n",
    "\n",
    "    def __next__(self) -> tuple[str, dict[str, str]]:\n",
    "        if self.current_paper_index >= self.iteration_count:\n",
    "            raise StopIteration\n",
    "        current_paper = self.paper_files[self.current_paper_index]\n",
    "        filepath = os.path.join(self.papers_folder, current_paper)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = (current_paper, json.load(f))\n",
    "        self.current_paper_index += 1\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "file_count = 0\n",
    "total_tokens = 0\n",
    "comment_tokens = 0\n",
    "ext_comment_count = 0\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "paper_iterator = PaperContentIterator(PAPERS_FOLDER)\n",
    "with tqdm(total=len(paper_iterator)) as pbar:\n",
    "    for name, files in paper_iterator:\n",
    "        if files is None:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        for file in files:\n",
    "            file_count += 1\n",
    "            ext_comments = extract_latex_comments(files[file])\n",
    "            ext_comment_count += len(ext_comments)\n",
    "            total_tokens += len(encoder.encode(files[file], disallowed_special=()))\n",
    "            comment_tokens += len(encoder.encode('\\n'.join(ext_comments), disallowed_special=()))\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Processing files={file_count} total_tokens={total_tokens} comment_tokens={comment_tokens} ext_comment_count={ext_comment_count}\")\n",
    "\n",
    "file_count, total_tokens, comment_tokens, ext_comment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most common comments to filter out boilerplate\n",
    "comment_counter = Counter()\n",
    "\n",
    "def cleanup_comments(comments: list[str]) -> list[str]:\n",
    "    comments = [comment.strip() for comment in comments] # Strip leading/trailing whitespace\n",
    "    comments = [re.sub(r'\\s+', ' ', comment) for comment in comments] # Normalize whitespace\n",
    "    comments = [comment for comment in comments if len(comment.strip()) > 5] # Remove short comments (including empty)\n",
    "    return comments\n",
    "\n",
    "paper_iterator = PaperContentIterator(PAPERS_FOLDER)\n",
    "with tqdm(total=len(paper_iterator), desc=\"Extracting comments\") as pbar:\n",
    "    for name, files in paper_iterator:\n",
    "        if files is None:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        for file in files:\n",
    "            comments = extract_latex_comments(files[file])\n",
    "            cleaned_comments = cleanup_comments(comments)\n",
    "            comment_counter.update(cleaned_comments)\n",
    "        pbar.update(1)\n",
    "        \n",
    "comment_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save common comments (that appear more than 10 times) to a file\n",
    "with open(COMMON_COMMENTS_TXT, 'w', encoding='utf-8') as f:\n",
    "    for comment, count in comment_counter.items():\n",
    "        if count > 10:\n",
    "            f.write(f\"{comment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d47702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into one big comments file\n",
    "remaining_comment_count = 0\n",
    "\n",
    "def cleanup_comments(comments: list[str]) -> list[str]:\n",
    "    comments = [comment.strip() for comment in comments] # Strip leading/trailing whitespace\n",
    "    comments = [re.sub(r'\\s+', ' ', comment) for comment in comments] # Normalize whitespace\n",
    "    comments = [comment for comment in comments if len(comment.strip()) > 5] # Remove short comments (including empty)\n",
    "    comments = [re.sub(r'%{4,}', '%%%', comment) for comment in comments] # Remove long separators\n",
    "    comments = [re.sub(r'-{4,}', '---', comment) for comment in comments] # Remove long separators\n",
    "    comments = [re.sub(r'={4,}', '===', comment) for comment in comments] # Remove long separators\n",
    "    return comments\n",
    "\n",
    "with open(COMMON_COMMENTS_TXT, 'r', encoding='utf-8') as f:\n",
    "    boilerplate = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "def filter_boilerplate(comments: list[str], boilerplate: set[str]) -> list[str]:\n",
    "    return [comment for comment in comments if comment not in boilerplate]\n",
    "\n",
    "paper_iterator = PaperContentIterator(PAPERS_FOLDER)\n",
    "with tqdm(total=len(paper_iterator), desc=\"Extracting comments\") as pbar:\n",
    "    with open(COMMENTS_JSONL, 'w', encoding='utf-8') as out_file:\n",
    "        for name, files in paper_iterator:\n",
    "            if files is None:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            merged_comments = ''\n",
    "            for file in files:\n",
    "                comments = extract_latex_comments(files[file])\n",
    "                cleaned_comments = filter_boilerplate(comments, boilerplate)\n",
    "                cleaned_comments = cleanup_comments(cleaned_comments)\n",
    "                remaining_comment_count += len(cleaned_comments)\n",
    "                merged_comments += '\\n'.join(cleaned_comments)\n",
    "            out_file.write(json.dumps({\"name\": name, \"comments\": merged_comments}) + '\\n')\n",
    "            pbar.update(1)\n",
    "            \n",
    "remaining_comment_count"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
