{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc378ccc",
   "metadata": {},
   "source": [
    "# LaTeXpOsEd: Data Mining Stage, Logical Filtering Substep\n",
    "\n",
    "In this stage the non-LaTeX files are filtered based on whether they are imported into the final PDF or not, as well as whether they are likely to contain any insteresting information.\n",
    "\n",
    "Before running this script:\n",
    "\n",
    "- Complete: [2_parse.ipynb](1_parse.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec025bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm pillow detect-secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import gzip\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS, GPSTAGS\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVES_DIR = 'data/archives'\n",
    "FILTERED_ARCHIVES_DIR = 'data/filtered_archives'\n",
    "CLEANED_ARCHIVES_DIR = 'data/cleaned_archives'\n",
    "LOG = 'data/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c6390",
   "metadata": {},
   "source": [
    "1. Extract each to the temp folder\n",
    "2. Perform filtering and logging\n",
    "3. Write decompressed files to filtered_archives\n",
    "4. Delete original tar to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039aecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Show everything\n",
    "    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "    filename=LOG,\n",
    ")\n",
    "\n",
    "#Auxiliary functions\n",
    "def has_gps_exif(img):\n",
    "    \"\"\"Check if image has GPS/location EXIF data.\"\"\"\n",
    "    try:\n",
    "        exif = img._getexif()\n",
    "        if not exif:\n",
    "            return False\n",
    "        for tag, value in exif.items():\n",
    "            decoded = TAGS.get(tag, tag)\n",
    "            if decoded == 'GPSInfo':\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def extract_gz_file(gz_path, dest_dir):\n",
    "    \"\"\"\n",
    "    Process a .gz file which may contain:\n",
    "      - A single text file (e.g., .tex)\n",
    "      - A compressed archive (e.g., a zip file or tarball)\n",
    "    Extracts images with GPS EXIF data, deletes others.\n",
    "    \"\"\"\n",
    "    archive_name = Path(gz_path).stem\n",
    "    subfolder_path = os.path.join(dest_dir, archive_name)\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "    with open(gz_path, 'rb') as f:\n",
    "        gz_data = f.read()\n",
    "\n",
    "    # Decompress GZ to get inner data\n",
    "    try:\n",
    "        with gzip.GzipFile(fileobj=io.BytesIO(gz_data)) as gz_file:\n",
    "            inner_data = gz_file.read()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to decompress {gz_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Try to treat decompressed data as ZIP\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(inner_data)) as zf:\n",
    "            logging.info(f\"{gz_path} contains a zip archive.\")\n",
    "\n",
    "            for member in zf.infolist():\n",
    "                if member.is_dir():\n",
    "                    continue\n",
    "\n",
    "                member_filename = os.path.basename(member.filename)\n",
    "                ext = Path(member_filename).suffix.lower()\n",
    "\n",
    "                with zf.open(member) as file:\n",
    "                    file_data = file.read()\n",
    "\n",
    "                    if ext in ['.jpg', '.jpeg', '.png']:\n",
    "                        try:\n",
    "                            img = Image.open(io.BytesIO(file_data))\n",
    "                            if has_gps_exif(img):\n",
    "                                logging.info(f\"GPS data found in image: {member.filename}\")\n",
    "                                # Save image\n",
    "                                output_path = os.path.join(subfolder_path, member_filename)\n",
    "                                with open(output_path, 'wb') as out_file:\n",
    "                                    out_file.write(file_data)\n",
    "                            else:\n",
    "                                logging.info(f\"Deleted image without GPS: {member.filename}\")\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Failed to process image: {member.filename} - {e}\")\n",
    "                            logging.info(f\"Deleted corrupted image: {member.filename}\")\n",
    "                        continue\n",
    "\n",
    "                    # Save non-image files\n",
    "                    output_path = os.path.join(subfolder_path, member_filename)\n",
    "                    with open(output_path, 'wb') as out_file:\n",
    "                        out_file.write(file_data)\n",
    "            return  # Zip processed successfully\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        logging.debug(f\"{gz_path} is not a zip archive\")\n",
    "\n",
    "    # Try to treat as a tar archive\n",
    "    try:\n",
    "        with tarfile.open(fileobj=io.BytesIO(inner_data)) as tf:\n",
    "            logging.info(f\"{gz_path} contains a tar archive.\")\n",
    "            tf.extractall(path=subfolder_path, filter=None)\n",
    "\n",
    "            # Now walk through contents for image deletion\n",
    "            for root, _, files in os.walk(subfolder_path):\n",
    "                for name in files:\n",
    "                    path = os.path.join(root, name)\n",
    "                    ext = Path(name).suffix.lower()\n",
    "                    if ext in ['.jpg', '.jpeg', '.png']:\n",
    "                        try:\n",
    "                            img = Image.open(path)\n",
    "                            if has_gps_exif(img):\n",
    "                                logging.info(f\"GPS data found in image: {name}\")\n",
    "                            else:\n",
    "                                os.remove(path)\n",
    "                                logging.info(f\"Deleted image without GPS: {name}\")\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Failed to open image: {name} - {e}\")\n",
    "                            os.remove(path)\n",
    "                            logging.info(f\"Deleted corrupted image: {name}\")\n",
    "            return  # Tar processed\n",
    "\n",
    "    except tarfile.TarError:\n",
    "        logging.debug(f\"{gz_path} is not a tar archive either\")\n",
    "\n",
    "    # Otherwise: assume it's a single file (e.g. .tex)\n",
    "    output_filename = archive_name\n",
    "    if not Path(output_filename).suffix:\n",
    "        output_filename += '.tex'\n",
    "\n",
    "    output_path = os.path.join(subfolder_path, output_filename)\n",
    "    with open(output_path, 'wb') as out_file:\n",
    "        out_file.write(inner_data)\n",
    "    logging.info(f\"Extracted single file from {gz_path} as {output_filename}\")\n",
    "\n",
    "def process_tar_file(tar_path):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        try:\n",
    "            with tarfile.open(tar_path, 'r') as tar:\n",
    "                tar.extractall(path=temp_dir, filter=None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to extract {tar_path}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Assume single folder inside\n",
    "        extracted_root = next(Path(temp_dir).iterdir())\n",
    "        if not extracted_root.is_dir():\n",
    "            logging.warning(f\"Expected a directory inside {tar_path}, found a file.\")\n",
    "            return\n",
    "\n",
    "        for item in extracted_root.iterdir():\n",
    "            if item.suffix.lower() == '.pdf':\n",
    "                logging.info(f\"Deleting PDF: {item.name}\")\n",
    "                item.unlink()  # Delete the PDF\n",
    "            elif item.suffix.lower() == '.gz':\n",
    "                try:\n",
    "                    extract_gz_file(str(item), temp_dir)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to process .gz file {item.name}: {e}\")\n",
    "                item.unlink()  # Delete original .gz after processing\n",
    "\n",
    "        # After processing all, copy temp_dir to final output\n",
    "        shutil.rmtree(extracted_root)\n",
    "        logging.info(f\"Removed original folder: {extracted_root}\")\n",
    "        \n",
    "        for item in Path(temp_dir).iterdir():\n",
    "            if item.is_dir():\n",
    "                if any(item.iterdir()):  # Folder is not empty\n",
    "                    dest = Path(FILTERED_ARCHIVES_DIR) / item.name\n",
    "                    shutil.copytree(item, dest, dirs_exist_ok=True)\n",
    "                    logging.info(f\"Copied folder to output: {dest}\")\n",
    "                else:\n",
    "                    logging.info(f\"Skipped empty folder: {item.name}\")\n",
    "            else:\n",
    "                logging.debug(f\"Skipping non-folder file in temp_dir: {item.name}\")\n",
    "\n",
    "        logging.info(f\"Finished processing {tar_path}\")\n",
    "\n",
    "\n",
    "# Cycle through all tar files in the archive directory\n",
    "def extract_and_process_tar_files(tar_dir):\n",
    "    for filename in os.listdir(tar_dir):\n",
    "        if filename.endswith('.tar'):\n",
    "            tar_path = os.path.join(tar_dir, filename)\n",
    "            print(f\"\\nExtracting {tar_path}\")\n",
    "            try:\n",
    "                process_tar_file(tar_path)\n",
    "                os.remove(tar_path)\n",
    "                logging.info(f\"Deleted tar file after processing: {filename}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "extract_and_process_tar_files(ARCHIVES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and summarize EXIF metadata from images\n",
    "\n",
    "def get_exif_data(image_path):\n",
    "    \"\"\"Extract EXIF data from an image file.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        exif_data = image._getexif()\n",
    "        if not exif_data:\n",
    "            return {}\n",
    "        exif = {}\n",
    "        for tag_id, value in exif_data.items():\n",
    "            tag = TAGS.get(tag_id, tag_id)\n",
    "            if tag == \"GPSInfo\":\n",
    "                gps_data = {}\n",
    "                for t in value:\n",
    "                    sub_tag = GPSTAGS.get(t, t)\n",
    "                    gps_data[sub_tag] = value[t]\n",
    "                exif[tag] = gps_data\n",
    "            else:\n",
    "                exif[tag] = value\n",
    "        return exif\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def has_location(exif):\n",
    "    \"\"\"Check if EXIF contains valid GPS Latitude and Longitude.\"\"\"\n",
    "    gps_info = exif.get('GPSInfo', {})\n",
    "    return 'GPSLatitude' in gps_info and 'GPSLongitude' in gps_info\n",
    "\n",
    "def has_device_info(exif):\n",
    "    \"\"\"Check if EXIF contains Make or Model.\"\"\"\n",
    "    return bool(exif.get('Make')) or bool(exif.get('Model'))\n",
    "\n",
    "def has_time_taken(exif):\n",
    "    \"\"\"Check if EXIF contains DateTimeOriginal or DateTime.\"\"\"\n",
    "    return bool(exif.get('DateTimeOriginal')) or bool(exif.get('DateTime'))\n",
    "\n",
    "def has_software(exif):\n",
    "    \"\"\"Check if EXIF contains Software tag.\"\"\"\n",
    "    return bool(exif.get('Software'))\n",
    "\n",
    "def find_images(root_folder, extensions=('jpg', 'jpeg', 'png')):\n",
    "    \"\"\"Recursively find image files with given extensions.\"\"\"\n",
    "    images = []\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(extensions):\n",
    "                images.append(os.path.join(dirpath, filename))\n",
    "    return images\n",
    "\n",
    "images = find_images(FILTERED_ARCHIVES_DIR)\n",
    "\n",
    "counts = {\n",
    "    \"Total Images\": len(images),\n",
    "    \"With GPS Location (Lat/Lon)\": 0,\n",
    "    \"With Device Info\": 0,\n",
    "    \"With Time Taken\": 0,\n",
    "    \"With Software\": 0\n",
    "}\n",
    "\n",
    "for img_path in images:\n",
    "    exif = get_exif_data(img_path)\n",
    "    if has_location(exif):\n",
    "        counts[\"With GPS Location (Lat/Lon)\"] += 1\n",
    "    if has_device_info(exif):\n",
    "        counts[\"With Device Info\"] += 1\n",
    "    if has_time_taken(exif):\n",
    "        counts[\"With Time Taken\"] += 1\n",
    "    if has_software(exif):\n",
    "        counts[\"With Software\"] += 1\n",
    "\n",
    "# Display the result as a DataFrame\n",
    "df_summary = pd.DataFrame(list(counts.items()), columns=[\"Metadata Type\", \"Count\"])\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd66b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_exif_data(exif_data):\n",
    "    \"\"\"Recursively convert all EXIF data values to strings for serialization.\"\"\"\n",
    "    if isinstance(exif_data, dict):\n",
    "        return {str(k): stringify_exif_data(v) for k, v in exif_data.items()}\n",
    "    elif isinstance(exif_data, (list, tuple)):\n",
    "        return [stringify_exif_data(v) for v in exif_data]\n",
    "    else:\n",
    "        return str(exif_data)\n",
    "\n",
    "def get_full_exif_data(image_path):\n",
    "    \"\"\"Extract all EXIF data from the image, with all values stringified.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        exif_data = image._getexif()\n",
    "        if not exif_data:\n",
    "            return {\"error\": \"No EXIF data found\"}\n",
    "        exif = {}\n",
    "        for tag_id, value in exif_data.items():\n",
    "            tag = TAGS.get(tag_id, tag_id)\n",
    "            if tag == \"GPSInfo\":\n",
    "                gps_data = {}\n",
    "                for t in value:\n",
    "                    sub_tag = GPSTAGS.get(t, t)\n",
    "                    gps_data[sub_tag] = value[t]\n",
    "                exif[tag] = gps_data\n",
    "            else:\n",
    "                exif[tag] = value\n",
    "        return stringify_exif_data(exif)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def find_images(root_folder, extensions=('jpg', 'jpeg', 'png')):\n",
    "    \"\"\"Recursively find image files with given extensions.\"\"\"\n",
    "    images = []\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(extensions):\n",
    "                images.append(os.path.join(dirpath, filename))\n",
    "    return images\n",
    "\n",
    "output_json_path = \"data/exif.json\"\n",
    "\n",
    "# Step 1: Find all images\n",
    "images = find_images(FILTERED_ARCHIVES_DIR)\n",
    "\n",
    "# Step 2: Collect all EXIF data\n",
    "exif_collection = {}\n",
    "\n",
    "for image_path in images:\n",
    "    rel_path = os.path.relpath(image_path, FILTERED_ARCHIVES_DIR)\n",
    "    exif_data = get_full_exif_data(image_path)\n",
    "    exif_collection[rel_path] = exif_data\n",
    "\n",
    "# Step 3: Save all EXIF data into a single JSON file\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(exif_collection, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ All EXIF data saved to: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be3245",
   "metadata": {},
   "source": [
    "## Filter out data that is imported into the latex document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_EXTENSIONS_TO_REMOVE = ['.pdf', '.png', '.jpg', '.jpeg', '.gif']\n",
    "INCLUDEGRAPHICS_PATTERN = re.compile(r'\\\\includegraphics(?:\\[[^\\]]*\\])?\\{([^}]+)\\}')\n",
    "INCLUDEPDF_PATTERN = re.compile(r'\\\\includepdf(?:\\[[^\\]]*\\])?\\{([^}]+)\\}')\n",
    "\n",
    "def extract_referenced_filenames(tex_content):\n",
    "    \"\"\"\n",
    "    Extract referenced filenames (without extensions) from non-commented lines of tex content\n",
    "    \"\"\"\n",
    "    referenced = set()\n",
    "    for line in tex_content.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith('%'):\n",
    "            continue\n",
    "        # Remove inline comments\n",
    "        line = line.split('%')[0]\n",
    "        referenced.update(INCLUDEGRAPHICS_PATTERN.findall(line))\n",
    "        referenced.update(INCLUDEPDF_PATTERN.findall(line))\n",
    "    return {os.path.splitext(os.path.basename(m))[0].lower() for m in referenced}\n",
    "\n",
    "def process_gz_file(file_path, output_folder):\n",
    "    original_name = os.path.basename(file_path)\n",
    "    base_name, ext = os.path.splitext(original_name)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        # === Try tar.gz processing ===\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with tarfile.open(fileobj=f_in, mode='r:*') as tar:\n",
    "                    tar.extractall(tempdir, filter=None)\n",
    "\n",
    "            # Tar.gz: extract references from tex files\n",
    "            referenced_files = set()\n",
    "            for root, _, files in os.walk(tempdir):\n",
    "                for f in files:\n",
    "                    if f.endswith('.tex'):\n",
    "                        tex_path = os.path.join(root, f)\n",
    "                        try:\n",
    "                            with open(tex_path, 'r', encoding='utf-8', errors='ignore') as tf:\n",
    "                                content = tf.read()\n",
    "                                referenced_files.update(extract_referenced_filenames(content))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reading {tex_path}: {e}\")\n",
    "\n",
    "            # Remove referenced images and pdfs\n",
    "            for root, _, files in os.walk(tempdir):\n",
    "                for f in files:\n",
    "                    name, ext = os.path.splitext(f)\n",
    "                    if ext.lower() in FILE_EXTENSIONS_TO_REMOVE:\n",
    "                        if name.lower() in referenced_files:\n",
    "                            try:\n",
    "                                os.remove(os.path.join(root, f))\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error deleting {f}: {e}\")\n",
    "\n",
    "            # Repack to .tar.gz\n",
    "            new_name = base_name + \"_filtered.tar.gz\"\n",
    "            output_path = os.path.join(output_folder, new_name)\n",
    "            with tarfile.open(output_path, \"w:gz\") as tar:\n",
    "                tar.add(tempdir, arcname=\".\")\n",
    "\n",
    "            return output_path\n",
    "\n",
    "        except (tarfile.ReadError, OSError):\n",
    "            # === Plain .gz file ===\n",
    "            try:\n",
    "                with gzip.open(file_path, 'rb') as f_in:\n",
    "                    decompressed_data = f_in.read()\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"{original_name} could not be decompressed: {e}\")\n",
    "\n",
    "            # Determine output name\n",
    "            if ext == '':\n",
    "                # No extension — assume .tex\n",
    "                new_name = base_name + \"_filtered.tex.gz\"\n",
    "            else:\n",
    "                # Preserve extension\n",
    "                new_name = base_name + \"_filtered\" + ext + \".gz\"\n",
    "\n",
    "            output_path = os.path.join(output_folder, new_name)\n",
    "\n",
    "            with gzip.open(output_path, 'wb') as f_out:\n",
    "                f_out.write(decompressed_data)\n",
    "\n",
    "            return output_path\n",
    "\n",
    "\n",
    "os.makedirs(CLEANED_ARCHIVES_DIR, exist_ok=True)\n",
    "all_files = [f for f in os.listdir(FILTERED_ARCHIVES_DIR) if os.path.isfile(os.path.join(FILTERED_ARCHIVES_DIR, f))]\n",
    "\n",
    "for filename in tqdm(all_files, desc=\"Processing files\", unit=\"file\"):\n",
    "        filepath = os.path.join(FILTERED_ARCHIVES_DIR, filename)\n",
    "\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            continue  # Skip PDFs\n",
    "        elif filename.lower().endswith('.gz'):\n",
    "            try:\n",
    "                process_gz_file(filepath, CLEANED_ARCHIVES_DIR)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nFailed to process {filename}: {e}\")\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_files_folder = 'data/interesting_files'\n",
    "interesting_extensions = {\n",
    "    \".xml\", \".txt\", \".csv\", \".out\", \".json\", \".md\", \".dat\",\n",
    "    \".py\", \".yaml\", \".bak\", \".docx\", \".ipynb\", \".stdout\",\n",
    "    \".strings\", \".yml\", \".db\", \".tmp\", \".html\", \".readme\"\n",
    "}\n",
    "\n",
    "\n",
    "def is_interesting(file_name):\n",
    "    name_lower = file_name.lower()\n",
    "    base_name, ext = os.path.splitext(name_lower)\n",
    "    return ext in interesting_extensions\n",
    "\n",
    "def make_flat_filename(rel_path):\n",
    "    # Replace OS-specific path separators with underscores\n",
    "    return rel_path.replace(os.sep, \"_\")\n",
    "\n",
    "def copy_flat_files(src_root, dst_root):\n",
    "    os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(src_root):\n",
    "        for file in filenames:\n",
    "            if is_interesting(file):\n",
    "                src_file_path = os.path.join(dirpath, file)\n",
    "                rel_path = os.path.relpath(src_file_path, src_root)\n",
    "                flat_filename = make_flat_filename(rel_path)\n",
    "\n",
    "                dst_file_path = os.path.join(dst_root, flat_filename)\n",
    "\n",
    "                # Avoid overwriting if duplicate paths produce same flat name\n",
    "                counter = 1\n",
    "                while os.path.exists(dst_file_path):\n",
    "                    name, ext = os.path.splitext(flat_filename)\n",
    "                    dst_file_path = os.path.join(dst_root, f\"{name}_{counter}{ext}\")\n",
    "                    counter += 1\n",
    "\n",
    "                try:\n",
    "                    shutil.copy2(src_file_path, dst_file_path)\n",
    "                    print(f\"Copied: {rel_path} ➜ {os.path.basename(dst_file_path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to copy {rel_path}: {e}\")\n",
    "\n",
    "# Run it\n",
    "copy_flat_files(CLEANED_ARCHIVES_DIR, interesting_files_folder)\n",
    "\n",
    "print(\"✅ Done copying interesting files to flat structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7e0d7",
   "metadata": {},
   "source": [
    "manual and LLM analysis from here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
