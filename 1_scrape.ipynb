{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c3adab",
   "metadata": {},
   "source": [
    "# LaTeXpOsEd: Scraping Stage\n",
    "\n",
    "In this stage, the map for arXiv's AWS S3 bucket map is downloaded and converted to an easily parsable JSON format. The content is also randomized for anonymity. Then, the files archives are downloaded, extracted and the relevant content saved one by one to not use too much disk space.\n",
    "\n",
    "Before running this script:\n",
    "\n",
    "- Install the AWS CLI tool on your local machine.\n",
    "- Create an AWS account and set up your local configuration to use the account with an access key.\n",
    "\n",
    "> ⚠️ It is important to note that the running of this script will incur charges on you AWS account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b554e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q boto3 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c952b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import random\n",
    "import tarfile\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "from boto3.s3.transfer import S3Transfer, TransferConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b645c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifest files\n",
    "ARXIV_MANIFEST_JSON = 'data/arXiv_src_manifest.json'\n",
    "ARXIV_MANIFEST_XML = 'data/arXiv_src_manifest.xml'\n",
    "# Number of papers to download. The actual number will be slightly higher due to the batching of achives.\n",
    "TARGET_COUNT = 100_000\n",
    "# Directory where the downloaded tar.gz files are stored.\n",
    "ARCHIVES_DIR = 'data/archives'\n",
    "# Temporary directory where the files will be extracted. It's best to mount a memory disk here.\n",
    "TMP_DIR = 'tmp'\n",
    "# Final directory where the selected files will be stored. One JSON for each paper.\n",
    "TARGET_DIR = 'data/final'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67bc35",
   "metadata": {},
   "source": [
    "## Download and parse storage map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_requester_pays(bucket, key, download_path):\n",
    "    transfer = S3Transfer(boto3.client('s3'), config=TransferConfig(use_threads=True))\n",
    "    transfer.download_file(\n",
    "        bucket, key, download_path,\n",
    "        extra_args={'RequestPayer': 'requester'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the original manifest file from arXiv in XML format\n",
    "download_requester_pays('arxiv', 'src/arXiv_src_manifest.xml', ARXIV_MANIFEST_XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON ordered randomly (or by timestamp?)\n",
    "\n",
    "# Parse XML file\n",
    "tree = ET.parse(ARXIV_MANIFEST_XML)\n",
    "root = tree.getroot()\n",
    "files = []\n",
    "for file_elem in root.findall('file'):\n",
    "    entry = {}\n",
    "    for child in file_elem:\n",
    "        entry[child.tag] = child.text\n",
    "    entry['num_items'] = int(entry['num_items'])\n",
    "    files.append(entry)\n",
    "\n",
    "# Sort by timestamp (ISO format: YYYY-MM-DD HH:MM:SS)\n",
    "# files.sort(key=lambda x: datetime.strptime(x['timestamp'], '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Randomize the order for anonymity\n",
    "random.shuffle(files)\n",
    "\n",
    "# Write to JSON\n",
    "with open(ARXIV_MANIFEST_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(files, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e85cda",
   "metadata": {},
   "source": [
    "## Download the archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "def s3_download_requester_pays(bucket, key, download_path):\n",
    "    transfer = S3Transfer(s3, config=TransferConfig(use_threads=True))\n",
    "    transfer.download_file(\n",
    "        bucket, key, download_path,\n",
    "        extra_args={'RequestPayer': 'requester'}\n",
    "    )\n",
    "    \n",
    "def extract_latex_from_archive(archive_path):\n",
    "    tex_data = {}\n",
    "    try: # Try extracting tar.gz content\n",
    "        with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and member.name.lower().endswith('.tex'):\n",
    "                    data = tar.extractfile(member)\n",
    "                    if data is not None:\n",
    "                        tex_data[member.name] = data.read().decode('utf-8', errors='ignore')\n",
    "    except Exception as e: # If it fails, try extracting gzip content\n",
    "        try:\n",
    "            with gzip.open(archive_path, 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "                tex_data['____main.tex'] = f.read()\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to extract {archive_path}: {e}, {e2}\")\n",
    "        return None\n",
    "    return tex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214380c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_files = []\n",
    "cumulative_items = 0\n",
    "\n",
    "# Clear tmp folder\n",
    "if os.path.exists(TMP_DIR):\n",
    "    for filename in os.listdir(TMP_DIR):\n",
    "        file_path = os.path.join(TMP_DIR, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "# Load manifest\n",
    "with open(ARXIV_MANIFEST_JSON, 'r', encoding='utf-8') as f:\n",
    "    files = json.load(f)\n",
    "\n",
    "\n",
    "# Select files until we reach the target count\n",
    "for fileinfo in files:\n",
    "    num_items = int(fileinfo['num_items'])\n",
    "    selected_files.append(fileinfo)\n",
    "    cumulative_items += num_items\n",
    "    if cumulative_items >= TARGET_COUNT:\n",
    "        break\n",
    "print(f'Selected {len(selected_files)} files, cumulative num_items={cumulative_items}')\n",
    "\n",
    "\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "os.makedirs(ARCHIVES_DIR, exist_ok=True)\n",
    "\n",
    "for fileinfo in tqdm(selected_files, desc='Downloading files'):\n",
    "    key = fileinfo['filename']  # \"src/arXiv_src_2505_191.tar\"\n",
    "    filename = os.path.basename(key)\n",
    "    download_path = os.path.join(ARCHIVES_DIR, filename)\n",
    "    \n",
    "    # Download from S3\n",
    "    s3_download_requester_pays('arxiv', key, download_path)\n",
    "    \n",
    "    # Extract main archive\n",
    "    with tarfile.open(download_path, 'r') as tar:\n",
    "        for member in tar.getmembers():\n",
    "            # Adjust the path to remove the top-level folder\n",
    "            member.name = os.path.basename(member.name)\n",
    "            tar.extract(member, path=TMP_DIR)\n",
    "    os.remove(download_path)\n",
    "\n",
    "    # Delete all folders and non-zip archives in the directory and keep files\n",
    "    for item in os.listdir(TMP_DIR):\n",
    "        item_path = os.path.join(TMP_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            os.rmdir(item_path)\n",
    "        elif not item.endswith('.gz'):\n",
    "            os.remove(item_path)\n",
    "            \n",
    "    for item in os.listdir(TMP_DIR):\n",
    "        paper_id = item.removesuffix('.gz')\n",
    "        item_path = os.path.join(TMP_DIR, item)\n",
    "        tex_data = extract_latex_from_archive(item_path)\n",
    "        with open(os.path.join(TARGET_DIR, f'{paper_id}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(tex_data, f)\n",
    "        os.remove(item_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
