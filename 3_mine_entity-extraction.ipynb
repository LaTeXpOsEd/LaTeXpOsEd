{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb45dde4",
   "metadata": {},
   "source": [
    "# LaTeXpOsEd: Data Mining Stage, Entity Extraction Substep\n",
    "\n",
    "In this step, a large language model is used via API calls to detect hidden information in the extracted comments.\n",
    "\n",
    "Before running this script:\n",
    "\n",
    "- Complete: [2_parse.ipynb](2_parse.ipynb)\n",
    "\n",
    "> ‚ö†Ô∏è It is important to note that the running of this script will incur charges with your LLM API account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854db7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, time, csv\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd  # kept for parity with original; not strictly required\n",
    "from openai import OpenAI\n",
    "from openai import APIError, RateLimitError, InternalServerError, BadRequestError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= CONFIG=========\n",
    "API_KEY        = \"ADD_YOUR_API_KEY\"\n",
    "MODEL_NAME     = \"google/gemini-2.5-flash\"\n",
    "#MODEL_NAME     = \"openai/gpt-4o\"\n",
    "INPUT_PATH      = \"data/paper_comments.jsonl\"\n",
    "LLM_PROVIDER    = \"https://openrouter.ai/api/v1\"\n",
    "OUTPUT_BASENAME = \"data/llm\" # writes out.json and out.csv\n",
    "TEMPERATURE     = 0.2\n",
    "MAX_RETRIES     = 4\n",
    "SLEEP_BACKOFF   = 2.0\n",
    "SHOW_COMMENTS   = True # True to include raw comments in CSV/JSON\n",
    "TEST_MODE_LIMIT = None # e.g., 10 for a quick smoke test\n",
    "MAX_WORKERS     = 50  # number of parallel threads for LLM calls, adjust based on rate limits\n",
    "# live SEM options\n",
    "COUNT_NONE_AS_HIT = True  # treat none/none as a hit in running stats, like original\n",
    "ALLOWED_LABELS = {\n",
    "    \"credentials\",\n",
    "    \"network_identifiers\",\n",
    "    \"pii\",\n",
    "    \"conflict\",\n",
    "    \"peerreview\",\n",
    "    \"none\",\n",
    "}\n",
    "SYSTEM_PROMPT_FILE = \"resources/system_prompt.md\"\n",
    "# ======================================\n",
    "\n",
    "# Resolve API key and init client\n",
    "API_KEY = API_KEY or os.getenv(\"OPENROUTER_API_KEY\") or os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise SystemExit(\"Please set API_KEY in config or OPENROUTER_API_KEY / OPENAI_API_KEY env var.\")\n",
    "\n",
    "client = OpenAI(base_url=LLM_PROVIDER, api_key=API_KEY)\n",
    "\n",
    "# Create results folder if needed\n",
    "os.makedirs(os.path.dirname(OUTPUT_BASENAME), exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete. Config loaded and OpenRouter client ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load system prompt\n",
    "\n",
    "with open(SYSTEM_PROMPT_FILE, \"r\") as f:\n",
    "    SYSTEM_PROMPT = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "XML_RE = re.compile(r\"<xml>.*?</xml>\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def build_prompt(comment_text: str) -> str:\n",
    "    return f\"{SYSTEM_PROMPT}\\n---\\n{comment_text}\\n---\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    m = XML_RE.search(text)\n",
    "    return m.group(0).strip() if m else None\n",
    "\n",
    "def sanitize_xml(xml_text: Optional[str]) -> Optional[str]:\n",
    "    if not xml_text:\n",
    "        return None\n",
    "    xml_text = xml_text.strip()\n",
    "    if xml_text.lower().startswith(\"<xml>\") and xml_text.lower().endswith(\"</xml>\"):\n",
    "        return xml_text\n",
    "    # salvage if wrapper missing\n",
    "    inner = re.sub(r\"^`+|`+$\", \"\", xml_text).strip()\n",
    "    inner = re.sub(r\"[^a-zA-Z0-9_,\\-\\s]\", \"\", inner).strip()\n",
    "    if inner:\n",
    "        return f\"<xml>{inner}</xml>\"\n",
    "    return None\n",
    "\n",
    "def parse_labels_from_xml(xml_text: str) -> List[str]:\n",
    "    # from <xml>a,b</xml> -> [\"a\", \"b\"] (normalized)\n",
    "    content = xml_text.strip()[5:-6].strip()\n",
    "    if not content:\n",
    "        return []\n",
    "    parts = [p.strip().lower() for p in content.split(\",\")]\n",
    "    labels = []\n",
    "    for p in parts:\n",
    "        p = p.replace(\"network_identifiers:\", \"network_identifiers\")\n",
    "        p = re.sub(r\"[^a-z_]\", \"\", p)\n",
    "        if not p:\n",
    "            continue\n",
    "        if p == \"none\":\n",
    "            return []\n",
    "        if p in ALLOWED_LABELS and p != \"none\":\n",
    "            labels.append(p)\n",
    "    # de-dup, preserve order\n",
    "    seen, out = set(), []\n",
    "    for l in labels:\n",
    "        if l not in seen:\n",
    "            seen.add(l)\n",
    "            out.append(l)\n",
    "    return out\n",
    "\n",
    "def parse_ground_truth_labels(rec: Dict[str, Any]) -> List[str]:\n",
    "    raw = rec.get(\"classification\", \"\")\n",
    "    if not isinstance(raw, str) or not raw.strip():\n",
    "        return []\n",
    "    parts = [p.strip().lower() for p in raw.split(\",\")]\n",
    "    labels = []\n",
    "    for p in parts:\n",
    "        p = p.replace(\"network_identifiers:\", \"network_identifiers\")\n",
    "        p = re.sub(r\"[^a-z_]\", \"\", p)\n",
    "        if p in ALLOWED_LABELS and p != \"none\":\n",
    "            labels.append(p)\n",
    "    seen, out = set(), []\n",
    "    for l in labels:\n",
    "        if l not in seen:\n",
    "            seen.add(l)\n",
    "            out.append(l)\n",
    "    return out\n",
    "\n",
    "def read_input_any(path: str) -> List[Dict[str, Any]]:\n",
    "    # supports JSON array, single JSON object, or NDJSON\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "        if not text:\n",
    "            return []\n",
    "        try:\n",
    "            data = json.loads(text)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            if isinstance(data, dict):\n",
    "                return [data]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    # NDJSON fallback\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                out.append(obj)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return out\n",
    "\n",
    "def ask_llm(comment_text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    prompt = build_prompt(comment_text)\n",
    "    backoff = SLEEP_BACKOFF\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = get_client().chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=TEMPERATURE,\n",
    "            )\n",
    "            content = resp.choices[0].message.content if resp.choices else \"\"\n",
    "            xml = extract_xml_answer(content)\n",
    "            xml = sanitize_xml(xml) or sanitize_xml(content)\n",
    "            if not xml:\n",
    "                return None, \"No valid <xml>...</xml> returned.\"\n",
    "            return xml, None\n",
    "        except (RateLimitError, InternalServerError) as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                return None, f\"{type(e).__name__}: {e}\"\n",
    "            time.sleep(backoff); backoff *= 1.7\n",
    "        except (APIError, BadRequestError) as e:\n",
    "            return None, f\"{type(e).__name__}: {e}\"\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                return None, f\"UnexpectedError: {e}\"\n",
    "            time.sleep(backoff); backoff *= 1.7\n",
    "    return None, \"Max retries exceeded\"\n",
    "\n",
    "def classify_record(idx_and_rec: Tuple[int, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    idx, rec = idx_and_rec\n",
    "    comment = rec.get(\"comments\", \"\")\n",
    "    if not isinstance(comment, str) or not comment.strip():\n",
    "        xml, err = \"<xml>none</xml>\", \"Missing or empty 'comments'.\"\n",
    "        pred_labels = []\n",
    "    else:\n",
    "        xml, err = ask_llm(comment)\n",
    "        pred_labels = parse_labels_from_xml(xml) if xml else []\n",
    "    return {\n",
    "        \"idx\": idx,\n",
    "        \"xml\": xml or \"\",\n",
    "        \"pred_labels\": list(set(pred_labels)),\n",
    "        \"error\": err,\n",
    "        \"_comment_length\": len(comment) if isinstance(comment, str) else 0,\n",
    "    }\n",
    "    \n",
    "# Thread-local OpenAI client (safe for multithreading with the SDK)\n",
    "_tls = threading.local()\n",
    "def get_client() -> OpenAI:\n",
    "    cli = getattr(_tls, \"client\", None)\n",
    "    if cli is None:\n",
    "        cli = OpenAI(base_url=LLM_PROVIDER, api_key=API_KEY)\n",
    "        _tls.client = cli\n",
    "    return cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run main classification inference\n",
    "\n",
    "records = read_input_any(INPUT_PATH)\n",
    "if TEST_MODE_LIMIT is not None:\n",
    "    records = records[:TEST_MODE_LIMIT]\n",
    "\n",
    "if not records:\n",
    "    raise SystemExit(\"No records found in input.\")\n",
    "\n",
    "# Phase 1: parallel classification WITH live SEM postfix\n",
    "results = [None] * len(records)\n",
    "\n",
    "# shared running metrics for live postfix\n",
    "lock = threading.Lock()\n",
    "sem_num_done = 0\n",
    "sem_num_exact = 0\n",
    "sem_num_hit_incl_none = 0\n",
    "sem_pred_label_freq = Counter()  # live tally (pred only), for postfix\n",
    "\n",
    "with tqdm(total=len(records), desc=\"Classifying (parallel)\", ncols=150, dynamic_ncols=True, leave=False) as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        fut_to_idx = {ex.submit(classify_record, (i, rec)): i for i, rec in enumerate(records)}\n",
    "        for fut in as_completed(fut_to_idx):\n",
    "            i = fut_to_idx[fut]\n",
    "            try:\n",
    "                r = fut.result()\n",
    "            except Exception as e:\n",
    "                r = {\"idx\": i, \"xml\": \"<xml>none</xml>\", \"pred_labels\": [], \"error\": f\"WorkerError: {e}\", \"_comment_length\": 0}\n",
    "            results[i] = r\n",
    "\n",
    "            # compute live metrics for this completed item\n",
    "            with lock:\n",
    "                pred_set = set(r[\"pred_labels\"])\n",
    "                gt_set   = set(parse_ground_truth_labels(records[i]))\n",
    "\n",
    "                sem_num_done += 1\n",
    "                if pred_set == gt_set:\n",
    "                    sem_num_exact += 1\n",
    "\n",
    "                inter = pred_set & gt_set\n",
    "                hit_now = bool(inter) or (COUNT_NONE_AS_HIT and not pred_set and not gt_set)\n",
    "                if hit_now:\n",
    "                    sem_num_hit_incl_none += 1\n",
    "\n",
    "                for l in pred_set:\n",
    "                    sem_pred_label_freq[l] += 1\n",
    "\n",
    "                running_exact = (sem_num_exact / sem_num_done) * 100.0\n",
    "                running_hit   = (sem_num_hit_incl_none / sem_num_done) * 100.0\n",
    "\n",
    "                postfix = {\n",
    "                    \"acc\":   f\"{running_exact:.3f}\",\n",
    "                    \"hit\":   f\"{running_hit:.3f}\",\n",
    "                    \"cred\":  sem_pred_label_freq[\"credentials\"],\n",
    "                    \"netid\": sem_pred_label_freq[\"network_identifiers\"],\n",
    "                    \"pii\":   sem_pred_label_freq[\"pii\"],\n",
    "                    \"conf\":  sem_pred_label_freq[\"conflict\"],\n",
    "                    \"prrev\": sem_pred_label_freq[\"peerreview\"],\n",
    "                    \"none\":  sem_pred_label_freq[\"none\"],  # typically 0; kept for parity\n",
    "                }\n",
    "                pbar.set_postfix(postfix)\n",
    "                pbar.update(1)\n",
    "\n",
    "# Phase 2: sequential aggregation, final metrics, and file outputs (same logic as original)\n",
    "enriched: List[Dict[str, Any]] = []\n",
    "\n",
    "num_done = 0\n",
    "num_exact = 0\n",
    "num_hit_incl_none = 0\n",
    "num_hit_nonempty = 0\n",
    "\n",
    "num_any_pred = 0\n",
    "num_any_gt = 0\n",
    "num_false_pos_records = 0\n",
    "num_false_neg_records = 0\n",
    "\n",
    "pred_label_freq = Counter()\n",
    "gt_label_freq = Counter()\n",
    "tp_label = Counter()\n",
    "fp_label = Counter()\n",
    "fn_label = Counter()\n",
    "\n",
    "for i, rec in enumerate(records):\n",
    "    r = results[i]\n",
    "    xml = r[\"xml\"]\n",
    "    pred_labels = r[\"pred_labels\"]\n",
    "    err = r[\"error\"]\n",
    "\n",
    "    gt_labels = parse_ground_truth_labels(rec)\n",
    "    pred_set, gt_set = set(pred_labels), set(gt_labels)\n",
    "\n",
    "    num_done += 1\n",
    "    if pred_set == gt_set:\n",
    "        num_exact += 1\n",
    "\n",
    "    inter = pred_set & gt_set\n",
    "    hit_now = bool(inter) or (COUNT_NONE_AS_HIT and not pred_set and not gt_set)\n",
    "    if hit_now:\n",
    "        num_hit_incl_none += 1\n",
    "    if inter:\n",
    "        num_hit_nonempty += 1\n",
    "\n",
    "    if pred_set: num_any_pred += 1\n",
    "    if gt_set:   num_any_gt += 1\n",
    "    if pred_set and not gt_set: num_false_pos_records += 1\n",
    "    if gt_set and not pred_set: num_false_neg_records += 1\n",
    "\n",
    "    for l in gt_set:   gt_label_freq[l] += 1\n",
    "    for l in pred_set: pred_label_freq[l] += 1\n",
    "    for l in inter: tp_label[l] += 1\n",
    "    for l in pred_set - gt_set: fp_label[l] += 1\n",
    "    for l in gt_set - pred_set: fn_label[l] += 1\n",
    "\n",
    "    row = {\n",
    "        **rec,\n",
    "        \"xml\": xml or \"\",\n",
    "        \"pred_labels\": list(pred_set),\n",
    "        \"gt_labels\": list(gt_set),\n",
    "        \"error\": err,\n",
    "    }\n",
    "    if not SHOW_COMMENTS:\n",
    "        row[\"_comment_length\"] = r[\"_comment_length\"]\n",
    "        row.pop(\"comments\", None)\n",
    "    enriched.append(row)\n",
    "\n",
    "print(f\"‚úÖ Finished.\")\n",
    "print(f\"Exact-match accuracy: {num_exact/num_done:.4f}\")\n",
    "print(f\"At least one correct (incl. none/none): {num_hit_incl_none} / {num_done} ({num_hit_incl_none/num_done:.1%})\")\n",
    "\n",
    "# Optional diagnostics (toggle if you want them)\n",
    "SHOW_NONEMPTY_COUNTS = False\n",
    "if SHOW_NONEMPTY_COUNTS:\n",
    "    print(f\"Ground-truth with ‚â•1 label: {num_any_gt} ({num_any_gt/num_done:.1%})\")\n",
    "    print(f\"Predictions with ‚â•1 label:  {num_any_pred} ({num_any_pred/num_done:.1%})\")\n",
    "    print(f\"At least one correct (non-empty only): {num_hit_nonempty} ({num_hit_nonempty/num_done:.1%})\")\n",
    "\n",
    "print(f\"False-positive records (pred‚â†‚àÖ & gt=‚àÖ): {num_false_pos_records}\")\n",
    "print(f\"False-negative records (gt‚â†‚àÖ & pred=‚àÖ): {num_false_neg_records}\")\n",
    "\n",
    "print(\"\\nBy-label counts (GT vs Pred, TP/FP/FN and precision/recall):\")\n",
    "labels_all = sorted(set(gt_label_freq) | set(pred_label_freq))\n",
    "for l in labels_all:\n",
    "    tp = tp_label[l]; fp = fp_label[l]; fn = fn_label[l]\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    print(f\" - {l:20s} GT={gt_label_freq[l]:4d}  Pred={pred_label_freq[l]:4d}  \"\n",
    "          f\"TP={tp:4d}  FP={fp:4d}  FN={fn:4d}  P={prec:.2f} R={rec:.2f}\")\n",
    "\n",
    "\n",
    "# save outputs (model name appended; replace \"/\" or \"\\\" with \"_\")\n",
    "model_safe = re.sub(r\"[\\\\/]\", \"_\", MODEL_NAME)  # e.g., \"qwen/qwen-2.5-7b-instruct\" -> \"qwen_qwen-2.5-7b-instruct\"\n",
    "base = f\"{OUTPUT_BASENAME}_{model_safe}\"\n",
    "json_path = base + \".json\"\n",
    "csv_path  = base + \".csv\"\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enriched, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "csv_cols = [\"xml\", \"pred_labels\", \"gt_labels\", \"error\"]\n",
    "if SHOW_COMMENTS:\n",
    "    csv_cols = [\"comments\"] + csv_cols\n",
    "else:\n",
    "    csv_cols = [\"_comment_length\"] + csv_cols\n",
    "\n",
    "with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"index\"] + csv_cols + [\"exact_match\"])\n",
    "    for idx, rec in enumerate(enriched):\n",
    "        exact = int(set(rec.get(\"pred_labels\", [])) == set(rec.get(\"gt_labels\", [])))\n",
    "        writer.writerow([\n",
    "            idx,\n",
    "            *(rec.get(c, \"\") if not isinstance(rec.get(c, \"\"), list) else \",\".join(rec.get(c, [])) for c in csv_cols),\n",
    "            exact\n",
    "        ])\n",
    "\n",
    "print(\"üìÑ Wrote files:\")\n",
    "print(\" -\", json_path)\n",
    "print(\" -\", csv_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
