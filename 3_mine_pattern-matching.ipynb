{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e59181d",
   "metadata": {},
   "source": [
    "# LaTeXpOsEd: Data Mining Stage, Pattern Matching Substep\n",
    "\n",
    "In this step, pattern matching techniques are used to extract information from the comments that were extracted from the papers. This includes:\n",
    "- URL extraction\n",
    "- Urlextract scan\n",
    "- Secret Patterns Database regex\n",
    "\n",
    "Before running this script:\n",
    "\n",
    "- Complete: [2_parse.ipynb](2_parse.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f921610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q urlextract tqdm pandas pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b673f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from math import ceil\n",
    "\n",
    "import yaml\n",
    "from urlextract import URLExtract\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_JSONL = 'data/paper_comments.jsonl'\n",
    "# Output files\n",
    "URLS_TXT = 'data/extracted_urls.txt'\n",
    "IPS_TXT = 'data/extracted_ips.txt'\n",
    "PUBLIC_IPS_TXT = 'data/extracted_public_ips.txt'\n",
    "IP_LOOKUP_CSV = 'data/ip_lookup.csv'\n",
    "# Configuration\n",
    "SECRETS_DB = 'resources/secrets-patterns-db-merged.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e13b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator class for convenient processing of the dataset\n",
    "class PaperExtractedCommentIterator:\n",
    "    def __init__(self, comments_file: str):\n",
    "        self.comments_file = comments_file\n",
    "        # Count lines\n",
    "        with open(comments_file, 'r', encoding='utf-8') as f:\n",
    "            self.iteration_count = sum(1 for line in f)\n",
    "        self.current_paper_index = 0\n",
    "        self.file_reader = open(comments_file, 'r', encoding='utf-8')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.iteration_count\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file_reader.close()\n",
    "\n",
    "    def __next__(self) -> tuple[int, dict[str, str]]:\n",
    "        if self.current_paper_index >= self.iteration_count:\n",
    "            self.file_reader.close()\n",
    "            raise StopIteration\n",
    "        content = self.current_paper_index, json.loads(self.file_reader.readline())\n",
    "        self.current_paper_index += 1\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23feaa1e",
   "metadata": {},
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c97e5c7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/paper_comments.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m urls = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m      3\u001b[39m url_extractor = URLExtract()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m paper_iterator = \u001b[43mPaperExtractedCommentIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMMENTS_JSONL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=\u001b[38;5;28mlen\u001b[39m(paper_iterator)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m index, comments \u001b[38;5;129;01min\u001b[39;00m paper_iterator:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mPaperExtractedCommentIterator.__init__\u001b[39m\u001b[34m(self, comments_file)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.comments_file = comments_file\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Count lines\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcomments_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.iteration_count = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.current_paper_index = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/Latexposed Public/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/paper_comments.jsonl'"
     ]
    }
   ],
   "source": [
    "# Extract URLs\n",
    "urls = set()\n",
    "url_extractor = URLExtract()\n",
    "paper_iterator = PaperExtractedCommentIterator(COMMENTS_JSONL)\n",
    "with tqdm(total=len(paper_iterator)) as pbar:\n",
    "    for index, comments in paper_iterator:\n",
    "        text = comments['comments']\n",
    "        url_list = url_extractor.find_urls(text)\n",
    "        urls.update(url_list)\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Extracting URLs: found_urls={len(urls)}\")\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = list(urls)\n",
    "with open(URLS_TXT, 'w', encoding='utf-8') as f:\n",
    "    for url in url_list:\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608678f5",
   "metadata": {},
   "source": [
    "## IP Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e161b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_pattern = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "ips = set()\n",
    "for url in urls:\n",
    "    match = ip_pattern.search(url)\n",
    "    if match:\n",
    "        ips.add(match.group())\n",
    "\n",
    "len(ips), ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7116be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_list = list(ips)\n",
    "with open(IPS_TXT, 'w', encoding='utf-8') as f:\n",
    "    for ip in ip_list:\n",
    "        f.write(ip + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f50d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify internal/private IPs\n",
    "internal_ip_pattern = re.compile(r'\\b(10\\.(?:[0-9]{1,3}\\.){2}[0-9]{1,3}|172\\.(1[6-9]|2[0-9]|3[0-1])\\.(?:[0-9]{1,3}\\.)[0-9]{1,3}|192\\.168\\.(?:[0-9]{1,3}\\.)[0-9]{1,3})\\b')\n",
    "internal_ips = [ip for ip in ips if internal_ip_pattern.search(ip)]\n",
    "len(internal_ips), internal_ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e87499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loopback IPs\n",
    "loopback_ips = [ip for ip in ips if ip.startswith('127.')]\n",
    "len(loopback_ips), loopback_ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e34771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External/Public IPs\n",
    "external_ips = [ip for ip in ips if ip not in internal_ips and ip not in loopback_ips]\n",
    "len(external_ips), external_ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PUBLIC_IPS_TXT, 'w', encoding='utf-8') as f:\n",
    "    for ip in external_ips:\n",
    "        f.write(ip + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf389f99",
   "metadata": {},
   "source": [
    "Use an IP lookup tool (for example: https://www.infobyip.com/ipbulklookup.php) and save it to `data/ip_lookup.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e68f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse lookup results\n",
    "headers = '\"IP\",\"Domain\",\"Country\",\"Region\",\"City\",\"ISP\",\"ASN\",\"Lat\",\"Long\",\"CNAME\"'\n",
    "\n",
    "# Read the lookup results file\n",
    "results_df = pd.read_csv(IP_LOOKUP_CSV, header=0)\n",
    "\n",
    "# Create a toplist of countries and ISPs\n",
    "country_toplist = results_df['Country'].value_counts()\n",
    "isp_toplist = results_df['ISP'].value_counts()\n",
    "\n",
    "country_toplist, isp_toplist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901388a",
   "metadata": {},
   "source": [
    "## Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain extractor\n",
    "def extract_domain(url):\n",
    "    match = re.search(r\"https?://([A-Za-z0-9.-]+)\", url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "tests = [\n",
    "    (\"http://example.com/path\", \"example.com\"),\n",
    "    (\"http://www.ieee.com\", \"www.ieee.com\"),\n",
    "    (\"http://www.dmlr.org/format/natbib.pdf\", \"www.dmlr.org\"),\n",
    "]\n",
    "\n",
    "for url, expected in tests:\n",
    "    assert extract_domain(url) == expected, f\"Failed for {url}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda60058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count domain occurences\n",
    "domain_counts = Counter()\n",
    "for url in urls:\n",
    "    domain = extract_domain(url)\n",
    "    if domain:\n",
    "        domain_counts[domain] += 1\n",
    "        \n",
    "domain_counts.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c864ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart with top 10 domains\n",
    "\n",
    "top_n = 10\n",
    "total_urls = sum(domain_counts.values())\n",
    "top_domains = domain_counts.most_common(top_n)\n",
    "\n",
    "labels = [d for d, _ in top_domains]\n",
    "counts = [c for _, c in top_domains]\n",
    "other_count = total_urls - sum(counts)\n",
    "#if other_count > 0:\n",
    "#    labels.append('Other')\n",
    "#    counts.append(other_count)\n",
    "\n",
    "# Clean and truncate labels for readability\n",
    "def clean_label(lbl: str) -> str:\n",
    "    return lbl[4:] if lbl.startswith('www.') else lbl\n",
    "\n",
    "def truncate(lbl: str, max_len: int = 28) -> str:\n",
    "    return lbl if len(lbl) <= max_len else lbl[: max_len - 1] + '…'\n",
    "\n",
    "clean_labels = [truncate(clean_label(l)) for l in labels]\n",
    "percents = [c / total_urls * 100 for c in counts]\n",
    "\n",
    "# Colors: palette for top domains, gray for Other\n",
    "base_colors = list(plt.cm.tab20.colors)\n",
    "colors = (base_colors * ceil(len(clean_labels) / len(base_colors)))[: len(clean_labels)]\n",
    "#if other_count > 0:\n",
    "#    colors[-1] = '#cccccc'  # gray for Other\n",
    "colors = [ colors[0] ] * len(top_domains)\n",
    "\n",
    "# Figure sizing based on number of bars\n",
    "height = max(6, 0.5 * len(clean_labels) + 1)\n",
    "fig, ax = plt.subplots(figsize=(8, height))\n",
    "\n",
    "ypos = list(range(len(clean_labels)))\n",
    "bars = ax.barh(ypos, counts, color=colors, edgecolor='white')\n",
    "ax.set_yticks(ypos)\n",
    "ax.set_yticklabels([''] * len(clean_labels))  # Hide default labels\n",
    "ax.invert_yaxis()  # highest values at the top\n",
    "\n",
    "# Annotate counts and percentages inside the bars (left side)\n",
    "for i, (bar, v, p) in enumerate(zip(bars, counts, percents)):\n",
    "    ax.text(\n",
    "        #bar.get_width() * 0.02, i,\n",
    "        25, i,\n",
    "        #f\"{v} ({p:.1f}%)\",\n",
    "        f\"{p:.1f}%\",\n",
    "        va='center', ha='left', color='white', fontsize=10, fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Annotate domain names to the right of the bars\n",
    "xmax = max(counts) if counts else 1\n",
    "for i, label in enumerate(clean_labels):\n",
    "    ax.text(\n",
    "        counts[i] + xmax * 0.01, i,\n",
    "        label,\n",
    "        va='center', ha='left', fontsize=10, color='black'\n",
    "    )\n",
    "\n",
    "ax.set_xlim(0, xmax * 1.15)\n",
    "\n",
    "# Title and footnote\n",
    "other_pct = (other_count / total_urls * 100) if total_urls else 0.0\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "plt.savefig('common_domains.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d8e93",
   "metadata": {},
   "source": [
    "## Custom search patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_search_patterns = {\n",
    "    # IBAN (International Bank Account Number)\n",
    "    'ibans': re.compile(r'\\b([A-Z]{2}\\d{2}[A-Z0-9]{11,30})\\b'),\n",
    "    # Generic bank account numbers (e.g., numeric sequences, 8–20 digits)\n",
    "    'bank_accounts': re.compile(r'\\b(\\d{8}-\\d{8}-?\\d{0,8})\\b'),\n",
    "    # SSH private key headers\n",
    "    'ssh_private_keys': re.compile(r'(-----BEGIN (?:RSA|DSA|EC|OPENSSH) PRIVATE KEY-----.*?-----END (?:RSA|DSA|EC|OPENSSH) PRIVATE KEY-----)', re.DOTALL),\n",
    "    # Generic API keys or secrets (alphanumeric tokens with length)\n",
    "    'api_keys': re.compile(r'\\b([A-Za-z0-9_\\-]{32,64})\\b'),\n",
    "    # Email addresses\n",
    "    'emails': re.compile(r'\\b([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,})\\b'),\n",
    "    # AWS access keys\n",
    "    'aws_access_keys': re.compile(r'\\b(AKIA[0-9A-Z]{16})\\b'),\n",
    "    # AWS secret keys\n",
    "    'aws_secret_keys': re.compile(r'\\b([0-9a-zA-Z/+]{40})\\b'),\n",
    "    # Credit card numbers (Visa, MasterCard, American Express)\n",
    "    'credit_cards': re.compile(r'\\b(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|6(?:011|5[0-9]{2})[0-9]{12}|(?:2131|1800|35\\d{3})\\d{11})\\b'),\n",
    "    # Social Security Numbers (SSNs) - US format\n",
    "    'us_ssns': re.compile(r'\\b(\\d{3}-\\d{2}-\\d{4})\\b'),\n",
    "    # JWT tokens\n",
    "    'jwt_tokens': re.compile(r'\\b(eyJ[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+)\\b'),\n",
    "    # Google API keys\n",
    "    'google_api_keys': re.compile(r'\\b(AIza[0-9A-Za-z\\-_]{35})\\b'),\n",
    "    # Github tokens\n",
    "    'github_tokens': re.compile(r'\\b(ghp_[A-Za-z0-9]{36})\\b'),\n",
    "    # Slack tokens\n",
    "    'slack_tokens': re.compile(r'\\b(xox[baprs]-[A-Za-z0-9]{10,48})\\b'),\n",
    "    # Phone numbers (various formats)\n",
    "    'phone_numbers': re.compile(r'\\b(\\+?\\d{1,3}?[-.\\s]??\\(?\\d{1,4}?\\)?[-.\\s]??\\d{1,4}[-.\\s]??\\d{1,9})\\b'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d900f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the custom pattern searches\n",
    "with open(COMMENTS_JSONL, \"r\") as f:\n",
    "    comments = f.read().splitlines()\n",
    "comments = [json.loads(line) for line in comments]\n",
    "\n",
    "findings = {key: set() for key in all_search_patterns.keys()}\n",
    "for c in tqdm(comments, desc=\"Pattern searching comments\"):\n",
    "    comment = c['comments']\n",
    "    for key, pattern in all_search_patterns.items():\n",
    "        if res := pattern.search(comment):\n",
    "            findings[key].add(res.group())\n",
    "        \n",
    "# Remove example.com emails\n",
    "findings['emails'] = {email for email in findings['emails'] if \"example.com\" not in email}\n",
    "\n",
    "# Save results\n",
    "os.makedirs('data/custom_patterns', exist_ok=True)\n",
    "for key, items in findings.items():\n",
    "    with open(f\"data/custom_patterns/{key}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(items))\n",
    "        print(f'{key}: {len(items)}')\n",
    "        \n",
    "emails = findings['emails']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a489f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count domain occurences in found emails\n",
    "email_domain_counts = Counter(email.split('@')[1].lower() for email in emails)\n",
    "email_domain_counts.most_common(15), sum(email_domain_counts.values()), len(email_domain_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top-level domain occurences in found emails\n",
    "email_top_domain_counts = Counter(email.split('@')[1].split('.')[-1].lower() for email in emails)\n",
    "email_top_domain_counts.most_common(15), len(email_top_domain_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748261d2",
   "metadata": {},
   "source": [
    "## Secrets Patterns DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e53d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdb_patterns = yaml.safe_load(open(SECRETS_DB))['patterns']\n",
    "\n",
    "findings = defaultdict(set)\n",
    "finding_count = 0\n",
    "for c in tqdm(comments, desc=\"Pattern searching comments\"):\n",
    "    comment = c['comments']\n",
    "    res = None\n",
    "    for pattern in sdb_patterns:\n",
    "        p = pattern['pattern']\n",
    "        name = p['name']\n",
    "        reg = p['regex']\n",
    "        confidence = p['confidence']\n",
    "        if confidence != 'low':\n",
    "            if res := re.search(reg, comment):\n",
    "                findings[name].add(res.group())\n",
    "                finding_count += 1\n",
    "\n",
    "# Save results\n",
    "import os\n",
    "os.makedirs('data/db_patterns', exist_ok=True)\n",
    "for key, items in findings.items():\n",
    "    with open(f\"data/db_patterns/{key}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(items))\n",
    "        print(f'{key}: {len(items)}')\n",
    "        \n",
    "finding_count, len(sdb_patterns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
